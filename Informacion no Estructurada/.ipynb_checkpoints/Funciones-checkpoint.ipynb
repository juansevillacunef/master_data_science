{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7137fe46",
   "metadata": {},
   "source": [
    "- Javier Concejo\n",
    "- Juan Sevilla\n",
    "- Yago Carbó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9826ac59",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp_es = spacy.load('es_core_news_lg')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71457294",
   "metadata": {},
   "source": [
    "# Formas canónicas y tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e149cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista=list(noticias['Texto'])\n",
    "\n",
    "def lema_canonica(texto):\n",
    "    noticias_doc=nlp(texto)\n",
    "    tokens_tagged = [(palabra.text, palabra.lemma_) for palabra in noticias_doc]\n",
    "    df=pd.DataFrame(tokens_tagged, columns=['Palabra', 'F. Canónica'])\n",
    "    return(df.iloc[:15])\n",
    "lema_canonica(lista[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c02a3",
   "metadata": {},
   "source": [
    "# Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_palabras_spacy(texto):\n",
    "    texto_tratado = texto.replace(\"\\n\", \" \")\n",
    "    doc = nlp(texto_tratado)\n",
    "    return(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2766da",
   "metadata": {},
   "source": [
    "# Frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad93b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Token_frases(texto):\n",
    "    doc = nlp(texto)\n",
    "    sentences = [sentence.text for sentence in doc.sents]\n",
    "    return (sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55cbd6",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords para varias filas\n",
    "lista_hotel_stopwords=list(hotel[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)])))\n",
    "\n",
    "#stopwords para un texto\n",
    "doc = nlp(lazarillo_text)\n",
    "def stopwords:\n",
    "    words = [word for word in doc if not word.is_stop]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba266d",
   "metadata": {},
   "source": [
    "# Traducción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8496a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "texto = \"In new lawsuits brought against the ride-sharing companies Uber and Lyft, the top prosecutors in Los Angeles\"\n",
    "\n",
    "def traduce(oracion, original, traducido):\n",
    "    input = TextBlob(oracion)\n",
    "    return(input.translate(from_lang=original, to=traducido))\n",
    "\n",
    "\n",
    "traduce(texto, 'en','es')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2a654",
   "metadata": {},
   "source": [
    "# TIFDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bafabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "lista=list(doc_definitivo['texto'])\n",
    "\n",
    "def tfidf_max_feature(datos):\n",
    "    TFIDF_vectorizer = TfidfVectorizer(max_features=20)\n",
    "    TFIDF_array=TFIDF_vectorizer.fit_transform(datos).toarray()\n",
    "    vocab_TFIDF = TFIDF_vectorizer.get_feature_names()\n",
    "    TFIDF=pd.DataFrame(TFIDF_array, columns=vocab_TFIDF)\n",
    "    return(TFIDF.head(5))\n",
    "tfidf_max_feature(lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a43629c",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55151872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "lista=list(doc_definitivo['texto'])\n",
    "\n",
    "def BOW_max_feature(datos):\n",
    "    BOW_vectorizer = CountVectorizer(max_features=20)\n",
    "    BOW_array=BOW_vectorizer.fit_transform(datos).toarray()\n",
    "    vocab_BOW = BOW_vectorizer.get_feature_names()\n",
    "    BOW=pd.DataFrame(BOW_array, columns=vocab_BOW)\n",
    "    return(BOW.head(5))\n",
    "BOW_max_feature(lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7402e25",
   "metadata": {},
   "source": [
    "# OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c81ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "lista=list(doc_definitivo['texto'])\n",
    "\n",
    "def OHE(datos):\n",
    "    OHE_vectorizer = CountVectorizer(max_features=20)\n",
    "    OHE_array=OHE_vectorizer.fit_transform(datos).toarray()\n",
    "    transformer = Binarizer().fit(OHE_array)\n",
    "    array2=transformer.transform(OHE_array)\n",
    "    vocab = OHE_vectorizer.get_feature_names()\n",
    "    OHE=pd.DataFrame(array2, columns=vocab)\n",
    "    return(OHE.head(5))\n",
    "OHE(lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988d930",
   "metadata": {},
   "source": [
    "# Categoría gramatical (POS) y Entidades (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6000055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "texto=sent_tokenize(texto)\n",
    "\n",
    "docum_analizado1=nlp_es(texto[0])\n",
    "\n",
    "tokens_1=[]\n",
    "dependencia_1=[]\n",
    "dep_1=[]\n",
    "tag_1=[]\n",
    "\n",
    "def dependencias(texto):\n",
    "    for token in texto:\n",
    "        tokens_1.append(token)\n",
    "        tag_1.append(token.tag_)\n",
    "        dep_1.append(token.dep_)\n",
    "        dependencia_1.append(spacy.explain(token.dep_))\n",
    "    \n",
    "    list_of_tuples_1 = list(zip(tokens_1,tag_1,dep_1,dependencia_1))\n",
    "    tabla_1 = pd.DataFrame(list_of_tuples_1,columns=['Tokens', 'Tag','Dep','Dependencia'])\n",
    "    return(tabla_1)\n",
    "\n",
    "dependencias(docum_analizado1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412317b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagrama con flechas\n",
    "doc = nlp(texto)\n",
    "for sent in doc.sents:\n",
    "    displacy.render(sent, jupyter=True, options={'distance': 110,'arrow_stroke': 2,'arrow_width': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colores\n",
    "for sent in esp_nlp.sents:\n",
    "    displacy.render(sent, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b904353",
   "metadata": {},
   "source": [
    "# Sintagmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp_es = spacy.load('es_core_news_lg')\n",
    "\n",
    "def get_sint_nom(doc):\n",
    "    df=pd.DataFrame(columns=[\"Sintagma\", \"Nucleo\"])\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        fila={\"Sintagma\":chunk.text, \"Nucleo\":chunk.root.text}\n",
    "        df = df.append(fila, ignore_index=True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613dc580",
   "metadata": {},
   "source": [
    "# Matriz de similitud coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "lista=list(doc_definitivo['texto'])\n",
    "\n",
    "def similitudes(texto):\n",
    "    TFIDF_vectorizer = TfidfVectorizer()\n",
    "    TFIDF_array=TFIDF_vectorizer.fit_transform(texto).toarray()\n",
    "    vocab_TFIDF = TFIDF_vectorizer.get_feature_names()\n",
    "    TFIDF=pd.DataFrame(TFIDF_array, columns=vocab_TFIDF)\n",
    "    similarity_matrix = cosine_similarity(TFIDF_array)\n",
    "    similarity_df = pd.DataFrame(similarity_matrix)\n",
    "    return(similarity_df)\n",
    "\n",
    "similitudes(lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b70c11",
   "metadata": {},
   "source": [
    "# Modelos de clasificación (Bayesiano, XGBoost, SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12072a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "modellist=['Bayesiano Ingenuo', 'SVC', 'XGBoost']\n",
    "disp_labels=['barato', 'Medio','Caro']\n",
    "\n",
    "def run_models(texto_tfidf, label, modellist, disp_labels = None):\n",
    "    \"\"\" \n",
    "        Este funcion entrena cada modelo definido en lista input, separa entre train y test,\n",
    "        predice las clases para el conjunto test y calcula el accuracy de entrenamiento y test,\n",
    "        asi como la matriz de confusion par cada modelo.\n",
    "        \n",
    "        Input: \n",
    "            texto_tfidf: matriz de TFIDF del Corpus\n",
    "            label: columna con el label de la clase de cada documento. Tienen que ser numéricas\n",
    "            modellist: lista de strings con los modelos a entrenar\n",
    "            disp_labels: labels de las clases para las matrices de confusion\n",
    "            \n",
    "        Output:\n",
    "            matriz de confusion, accuacy de entrenaiento, test y fiabilidad para cada modelo entrenado\n",
    "    \"\"\"\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "    \n",
    "    texto_train, texto_test, label_train, label_test = train_test_split(texto_tfidf, label, train_size = 0.8,\\\n",
    "                                                                        random_state=100, stratify=label)\n",
    "    \n",
    "    for name in modellist:\n",
    "        if name == \"Bayesiano Ingenuo\":\n",
    "            model = MultinomialNB()\n",
    "        if name == \"SVC\":\n",
    "            model = SVC(kernel='linear')\n",
    "        if name == \"XGBoost\":\n",
    "            model = XGBClassifier(n_jobs = -1, random_state = 100)\n",
    "\n",
    "\n",
    "        model.fit(texto_train, label_train)\n",
    "        categs_pred = model.predict(texto_test)\n",
    "        acc_train = model.score(texto_train, label_train)\n",
    "        acc_test = model.score(texto_test, label_test)\n",
    "        ConfusionMatrixDisplay.from_predictions(label_test, categs_pred, display_labels = disp_labels);\n",
    "        \n",
    "\n",
    "        # Confusion Matrix\n",
    "        print('Para el modelo','\\033[1m' + str(name) + '\\033[0m',\"matriz de confusion es:\")\n",
    "        plt.show()\n",
    "        print(\"Con un accuracy de train de:\", round(acc_train*100,2),\"%\")\n",
    "        print(\"Con un accuracy de test de:\", round(acc_test*100,2),\"%\")\n",
    "        print(\"Con una fiabilidad de:\", round(acc_test/acc_train*100,2),\"%\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ecf05e",
   "metadata": {},
   "source": [
    "# Clasificación con transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e597ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "## Análisis de sentimientos\n",
    "model = \"finiteautomata/beto-sentiment-analysis\"\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model)\n",
    "print(sentiment_pipeline([\"Un sensacional partido de baloncesto\", \"la peor película premiada en la gala\"]))\n",
    "\n",
    "## Natural Language Inference (¿Está contenida la respuesta a esta pregunta??)\n",
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder('cross-encoder/qnli-electra-base')\n",
    "score = model.predict([(\"Where is the capital of France?\",\"Paris is the capital of France.\")])\n",
    "print(score)\n",
    "\n",
    "## Zero-Shot-Classification\n",
    "model = \"Recognai/zeroshot_selectra_medium\"\n",
    "clasif = pipeline(\"zero-shot-classification\", model=model)\n",
    "print(clasif(\n",
    "    \"El IPC sube de golpe y se dispara hasta el 9,8% en marzo, su tasa más alta desde 1985\",\n",
    "    candidate_labels=[\"política\", \"deportes\", \"economía\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e40ede1",
   "metadata": {},
   "source": [
    "# QA con transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652635e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = \"mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\"\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model)\n",
    "\n",
    "Contexto = \"La montaña mágica (Der Zauberberg, en el original alemán) es una novela de Thomas Mann que se publicó \" \\\n",
    "\"en 1924. Es considerada la novela más importante de su autor y un clásico de la literatura en lengua alemana\" \\\n",
    "\"del siglo XX que ha sido traducido a numerosos idiomas, siendo dominio público en países como Estados Unidos, \" \\\n",
    "\"España, Brasil, entre otros. \" \\\n",
    "\"Thomas Mann comenzó a escribir la novela en 1912, a raíz de una visita a su esposa en el Sanatorio Wald de Davos \" \\\n",
    "\"en el que se encontraba internada. La concibió inicialmente como una novela corta, pero el proyecto fue creciendo \" \n",
    "\"con el tiempo hasta convertirse en una obra mucho más extensa. La obra narra la estancia de su protagonista principal, \" \\\n",
    "\"el joven Hans Castorp, en un sanatorio de los Alpes suizos al que inicialmente había llegado únicamente como \" \\\n",
    "\"visitante. La obra ha sido calificada de novela filosófica, porque, aunque se ajusta al molde genérico del \" \\\n",
    "\"Bildungsroman o novela de aprendizaje, introduce reflexiones sobre los temas más variados, tanto a cargo del narrador\" \\\n",
    "\"como de los personajes (especialmente Naphta y Settembrini, los encargados de la educación del protagonista).\"\n",
    "\n",
    "print(qa_pipeline(question = \"¿Cuándo se escribió la Montaña Magica?\", context= Contexto))\n",
    "print(qa_pipeline(question = \"¿Cómo se titula La Montaña Mágica originalmente?\", context= Contexto))\n",
    "print(qa_pipeline(question = \"¿En qué idioma está escrita La Montaña Mágica?\", context= Contexto))\n",
    "\n",
    "print(qa_pipeline(question = \"Who wrote Der Zauerberg?\", context= Contexto))\n",
    "print(qa_pipeline(question = \"Where was Thomas Mann wife staying?\", context= Contexto))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a511d2",
   "metadata": {},
   "source": [
    "# Traducción con transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "\n",
    "translator = pipeline(\"translation\", model=model)\n",
    "\n",
    "translator(\"How many roads must a man walk down \" \\\n",
    "\"Before you call him a man? \" \\\n",
    "\"How many seas must a white dove sail\" \\\n",
    "\"Before she sleeps in the sand? \" \\\n",
    "\"Yes, and how many times must the cannonballs fly \" \\\n",
    "\"Before they're forever banned?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05b53e",
   "metadata": {},
   "source": [
    "# Resúmenes con transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61151512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel, AutoTokenizer\n",
    "\n",
    "text = \"La montaña mágica (Der Zauberberg, en el original alemán) es una novela de Thomas Mann que se publicó \" \\\n",
    "\"en 1924. Es considerada la novela más importante de su autor y un clásico de la literatura en lengua alemana\" \\\n",
    "\"del siglo XX que ha sido traducido a numerosos idiomas, siendo dominio público en países como Estados Unidos, \" \\\n",
    "\"España, Brasil, entre otros. \" \\\n",
    "\"Thomas Mann comenzó a escribir la novela en 1912, a raíz de una visita a su esposa en el Sanatorio Wald de Davos \" \\\n",
    "\"en el que se encontraba internada. La concibió inicialmente como una novela corta, pero el proyecto fue creciendo \" \n",
    "\"con el tiempo hasta convertirse en una obra mucho más extensa. La obra narra la estancia de su protagonista principal, \" \\\n",
    "\"el joven Hans Castorp, en un sanatorio de los Alpes suizos al que inicialmente había llegado únicamente como \" \\\n",
    "\"visitante. La obra ha sido calificada de novela filosófica, porque, aunque se ajusta al molde genérico del \" \\\n",
    "\"Bildungsroman o novela de aprendizaje, introduce reflexiones sobre los temas más variados, tanto a cargo del narrador\" \\\n",
    "\"como de los personajes (especialmente Naphta y Settembrini, los encargados de la educación del protagonista).\"\n",
    "\n",
    "print(\"Text:\")\n",
    "print(text)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"IIC/marimari-r2r-mlsum\")\n",
    "model = EncoderDecoderModel.from_pretrained(\"IIC/marimari-r2r-mlsum\")\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "output_ids = model.generate(input_ids)[0]\n",
    "\n",
    "print(\"Resumen:\")\n",
    "print(tokenizer.decode(output_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1de582",
   "metadata": {},
   "source": [
    "# Generación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb47e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model = 'datificate/gpt2-small-spanish')\n",
    "generator(\"Hace muchos años em las Tierras Altas de Escocia\", max_length = 50, num_return_sequences=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
