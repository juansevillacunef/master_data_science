{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminación de palabras vacías (stop-words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'es_core_news_lg'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/41/_wb9xl2j5mx33x3_7jr_cvx00000gp/T/ipykernel_7446/1227301203.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m### carga el modelo es_core_news_lg de spacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es_core_news_lg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'es_core_news_lg'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "### Importación de librerías. Importa nltk y spacy\n",
    "\n",
    "import time\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# cargamos modelo de tokenizacion para nltk, y de stopwords\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/spanish.pickle\")\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "### carga el modelo es_core_news_lg de spacy\n",
    "nlp = spacy.load(\"es_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Genera una función que lea un fichero txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos a seguir con NLTK\n",
    "1. Tokenización del texto (separar en palabras)\n",
    "2. Filtrado de la lista de palabras, para excluir aquellas que estén en la lista de stopwords (generada en la primera celda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pues', 'sepa', 'V.M', '.', 'ante', 'todas', 'cosas', 'que', 'a', 'mí', 'llaman', 'Lázaro', 'de', 'Tormes', ',', 'hijo', 'de', 'Tomé', 'González', 'y', 'de', 'Antona', 'Pérez', ',', 'naturales', 'de', 'Tejares', ',', 'aldea', 'de']\n",
      "['Pues', 'sepa', 'V.M', '.', 'todas', 'cosas', 'llaman', 'Lázaro', 'Tormes', ',', 'hijo', 'Tomé', 'González', 'Antona', 'Pérez', ',', 'naturales', 'Tejares', ',', 'aldea', 'Salamanca', '.', 'nacimiento', 'dentro', 'río', 'Tormes', ',', 'causa', 'tomé', 'sobrenombre']\n"
     ]
    }
   ],
   "source": [
    "### Principal. Lee el archivo de lazarillo\n",
    "\n",
    "lazarillo_text = read_text_file(\"data/Lazarillo.txt\")\n",
    "\n",
    "# tokeniza el texto con nltk\n",
    "words = nltk.word_tokenize(lazarillo_text)\n",
    "print(words[:30])\n",
    "\n",
    "# filtra la lista de palabras para excluir las que son stop_words\n",
    "words = [word for word in words if word.lower() not in stopwords]\n",
    "print(words[:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos a seguir con Spacy\n",
    "1. Generación del doc (si recorremos doc con un for, ya tenemos los tokens).\n",
    "2. Filtrado de la lista de palabras, para excluir aquellas que tengan la etiqueta de stop word (en cada palabra del texto, .is_stop es True si lo es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sepa, V.M., \n",
      ", a, llaman, Lázaro, Tormes, ,, hijo, Tomé, González, y, Antona, Pérez, ,, naturales, Tejares, ,, aldea, Salamanca, ., \n",
      ", nacimiento, río, Tormes, ,, causa, tomé, sobrenombre, ,]\n"
     ]
    }
   ],
   "source": [
    "### Eliminación stopwords - spaCy\n",
    "\n",
    "# genera el doc \n",
    "doc = nlp(lazarillo_text)\n",
    "\n",
    "# filtra el listado de palabras utilizando .is_stop\n",
    "words = [word for word in doc if not word.is_stop]\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "107fb03afb2754bdb3cdbb13c1c83d7d6037442339c22e5ee8cf40869e8513c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
